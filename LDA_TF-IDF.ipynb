{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68cd23fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (63.4.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (1.24.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (8.2.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.8.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78b04ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2f80282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: gensim in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyLDAvis) (4.1.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.11.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.9.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.3.2)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.0.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyLDAvis) (63.4.1)\n",
      "Requirement already satisfied: funcy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.0)\n",
      "Requirement already satisfied: numexpr in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.8.3)\n",
      "Requirement already satisfied: pandas>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.1.3)\n",
      "Requirement already satisfied: numpy>=1.24.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.24.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2022.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->pyLDAvis) (2.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->pyLDAvis) (2.0.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\anaconda3\\lib\\site-packages (from numexpr->pyLDAvis) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from packaging->numexpr->pyLDAvis) (3.0.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b0b76cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python setup.py build_ext\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e65cdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91581a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "527b9564",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa77b8c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lauren boebert calling abolishment dept education like chlamydia speaking out against dang\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_excel('Drugs.xlsx')['New Tweet']\n",
    "# get first 90 characters\n",
    "print(data[0][0:90])\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0ba056d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call dept education chlamydia speak danger\n"
     ]
    }
   ],
   "source": [
    "# import en_core_web_sm\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "#     nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    texts_out = []\n",
    "    for text in texts:\n",
    " # Handle NaN values\n",
    "        if pd.isna(text):\n",
    "            text = ''\n",
    "        doc = nlp(text)\n",
    "        new_text = []\n",
    "        for token in doc:\n",
    "            if token.pos_ in allowed_postags:\n",
    "                new_text.append(token.lemma_)\n",
    "        final = ' '.join(new_text)\n",
    "        texts_out.append(final)\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "lemmatized_texts = lemmatization(data)\n",
    "print(lemmatized_texts[0][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ee87531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['call', 'dept', 'education', 'chlamydia', 'speak', 'danger']\n"
     ]
    }
   ],
   "source": [
    "def gen_words(texts):\n",
    "    final = []\n",
    "    for text in texts:\n",
    "        new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "        final.append(new)\n",
    "    return (final)\n",
    "\n",
    "data_words = gen_words(lemmatized_texts)\n",
    "print(data_words[0][0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd2403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bigrams and Trigrams\n",
    "bigram_phrases = gensim.models.Phrases(data_words, min_count=7, threshold=50)\n",
    "trigram_phrases = gensim.models.Phrases(bigram_phrases[data_words], threshold=50)\n",
    "\n",
    "bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "# Function to make bigrams\n",
    "def make_bigrams(texts):\n",
    "    return [bigram[doc] for doc in texts]\n",
    "\n",
    "# Function to make trigrams\n",
    "def make_trigram(texts):\n",
    "    return [trigram[bigram[doc]] for doc in texts]\n",
    "\n",
    "data_bigrams = make_bigrams(data_words)\n",
    "data_bigrams_trigrams = make_trigram(data_bigrams)\n",
    "\n",
    "# Convert generators to lists\n",
    "data_bigrams_list = list(data_bigrams)\n",
    "data_bigrams_trigrams_list = list(data_bigrams_trigrams)\n",
    "\n",
    "# Check if there are non-empty lists before printing\n",
    "if data_bigrams_trigrams_list:\n",
    "    print(data_bigrams_trigrams_list[0])\n",
    "else:\n",
    "    print(\"No non-empty bigrams and trigrams.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dbdfb44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF REMOVAL\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "id2word = corpora.Dictionary(data_bigrams_trigrams)\n",
    "\n",
    "texts = data_bigrams_trigrams\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "print(corpus[0][0:10])\n",
    "\n",
    "tfidf = TfidfModel(corpus, id2word=id2word)\n",
    "\n",
    "low_value = 0.03\n",
    "words =[]\n",
    "words_missing_in_tfidf = []\n",
    "for i in range(0, len(corpus)):\n",
    "    bow = corpus[i]\n",
    "    low_value_words = []\n",
    "    tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "    bow_ids = [id for id, value in bow]\n",
    "    low_value_words = [id for pid, value in tfidf[bow] if value < low_value]\n",
    "    drops = low_value_words + words_missing_in_tfidf\n",
    "    for item in drops:\n",
    "        words.append(id2word[item])\n",
    "    words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids] #the words with tfidf score 0 will be missing\n",
    "    \n",
    "    new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "    corpus[i] = new_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "be60db51",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model2 = gensim.models.LdaMulticore(corpus, \n",
    "                                   num_topics = 5, \n",
    "                                   id2word = id2word,                                    \n",
    "                                   passes = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e7e81e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.034*\"amoxicillin\" + 0.014*\"take\" + 0.011*\"lorazepam\" + 0.008*\"antibiotic\" + 0.008*\"make\" + 0.007*\"go\" + 0.007*\"get\" + 0.007*\"buy\" + 0.006*\"say\" + 0.006*\"patient\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.025*\"lorazepam\" + 0.017*\"get\" + 0.015*\"amoxicillin\" + 0.012*\"give\" + 0.009*\"use\" + 0.008*\"here\" + 0.007*\"infection\" + 0.006*\"treatment\" + 0.006*\"year\" + 0.005*\"make\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.019*\"amoxicillin\" + 0.013*\"lorazepam\" + 0.013*\"take\" + 0.013*\"get\" + 0.011*\"go\" + 0.011*\"day\" + 0.010*\"use\" + 0.010*\"nurofen\" + 0.009*\"covid\" + 0.008*\"week\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.016*\"get\" + 0.016*\"take\" + 0.015*\"amoxicillin\" + 0.015*\"so\" + 0.011*\"lorazepam\" + 0.009*\"sleep\" + 0.009*\"say\" + 0.008*\"give\" + 0.008*\"go\" + 0.007*\"drug\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.036*\"lorazepam\" + 0.025*\"take\" + 0.013*\"just\" + 0.011*\"need\" + 0.010*\"get\" + 0.009*\"work\" + 0.009*\"so\" + 0.008*\"now\" + 0.008*\"infection\" + 0.007*\"prescribe\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "\n",
    "for idx, topic in lda_model2.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02bdbee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1207222856979607528758286975\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1207222856979607528758286975_data = {\"mdsDat\": {\"x\": [0.2016196085480884, 0.07343851820588836, -0.1730104300849236, -0.13938317373512327, 0.03733547706607018], \"y\": [-0.013788180714299634, 0.17369324916730328, -0.10680049919611133, 0.12066246023056082, -0.17376702948745315], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [25.351665994312256, 24.047779149842253, 22.690616324128317, 15.389592273916305, 12.52034625780087]}, \"tinfo\": {\"Term\": [\"so\", \"give\", \"take\", \"need\", \"patient\", \"patient\", \"try\", \"thank\", \"hour\", \"diazepam\", \"give\", \"too\", \"say\", \"tell\", \"take\", \"lorazepam\", \"time\", \"very\", \"also\", \"much\", \"test\", \"so\", \"pain\", \"take\", \"lorazepam\", \"day\", \"just\", \"dose\", \"treatment\", \"look\", \"find\", \"short\", \"help\", \"need\", \"good\", \"really\", \"amoxicillin\", \"lorazepam\", \"take\", \"people\", \"stop\", \"side_effect\", \"link\", \"hope\", \"use\", \"lorazepam\", \"so\", \"leave\", \"child\", \"shit\", \"talk\", \"far\", \"bit\", \"get\", \"today\", \"just\", \"need\"], \"Freq\": [40.0, 39.0, 92.0, 28.0, 21.0, 20.381400570027562, 9.711146007366725, 9.65993593686473, 9.548544012012707, 8.893042709311162, 34.988468974204395, 18.47616937223587, 23.35711076233866, 13.348047510212922, 31.756200754717405, 31.49561325171678, 16.135500430385267, 11.884730262014534, 10.527086792267523, 8.784882196008951, 7.979663610117365, 30.26624841783918, 14.684030602492534, 42.77636894450384, 51.06014921050983, 16.874607332716533, 18.066901426144153, 12.648056777669069, 11.823655979635184, 10.928130256042472, 9.40138661449686, 8.699697603827271, 20.660693644431067, 21.590028569795404, 16.22633996958735, 15.169999937675119, 21.409605869484047, 27.2709854358566, 17.22664817524629, 9.244387988661385, 8.308289358279577, 7.277158387028348, 6.936210141682303, 6.621411459988584, 12.819500493400051, 14.668447403947118, 9.568443284975235, 6.198977379913085, 5.657829004220839, 5.010341355250694, 5.3131060968946935, 4.568786058173464, 4.783729909714032, 11.475286823085707, 5.515734474788143, 6.9659250019912715, 6.211554794607692], \"Total\": [40.0, 39.0, 92.0, 28.0, 21.0, 21.00996677692945, 10.31026951621764, 10.2583448682767, 10.151377970092366, 9.486297579970126, 39.67615173339357, 22.33418807172494, 31.9626009074822, 16.38843975909146, 92.04507109932821, 124.7272233791931, 16.73911586558043, 12.499162830344547, 11.140815287217174, 9.410972104332792, 8.596547733944021, 40.30244940634864, 17.608617654995548, 92.04507109932821, 124.7272233791931, 28.110743442163358, 36.4687061280582, 13.248132793879522, 12.442704227773495, 11.524677461286219, 10.002519240490457, 9.334167796186495, 25.774856253253745, 28.272969557072386, 20.29410604449849, 18.519689175245365, 56.778988283839745, 124.7272233791931, 92.04507109932821, 9.905493509109611, 8.925076347121768, 7.888145509698096, 7.538638357839546, 7.271697257583548, 37.659295601781196, 124.7272233791931, 40.30244940634864, 6.809441768854996, 6.280360608508283, 5.621834756513399, 5.967346204234945, 5.22424524244343, 5.509970786972464, 58.99665145427292, 8.266627425181897, 36.4687061280582, 28.272969557072386], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [5.0, 4.0, 3.0, 2.0, 1.0, -4.1695, -4.9108, -4.9161, -4.9277, -4.9988, -3.6291, -4.2676, -4.0332, -4.5927, -3.726, -3.7342, -4.3503, -4.656, -4.7773, -4.9583, -5.0544, -3.7213, -4.4445, -3.3753, -3.1983, -4.3055, -4.2372, -4.5357, -4.6031, -4.6819, -4.8323, -4.9099, -4.045, -4.001, -4.2866, -4.3539, -4.0094, -3.7674, -4.2267, -4.4609, -4.5677, -4.7002, -4.7482, -4.7946, -4.134, -3.9992, -4.4265, -4.6542, -4.7456, -4.8671, -4.8084, -4.9594, -4.9134, -4.0384, -4.771, -4.5376, -4.6522], \"loglift\": [5.0, 4.0, 3.0, 2.0, 1.0, 1.342, 1.3125, 1.3122, 1.3111, 1.3077, 1.2466, 1.1827, 1.0587, 1.1671, 0.3081, -0.004, 1.3884, 1.3747, 1.3685, 1.3563, 1.3507, 1.1387, 1.2435, 0.6588, 0.532, 0.9148, 0.7228, 1.4369, 1.4322, 1.4301, 1.4212, 1.4128, 1.2621, 1.2135, 1.2595, 1.2837, 0.5079, -0.0371, -0.1926, 1.8024, 1.7999, 1.7909, 1.7882, 1.7778, 0.7939, -0.269, 0.4335, 1.9839, 1.9734, 1.9627, 1.9617, 1.9438, 1.9365, 0.4405, 1.6732, 0.4224, 0.5623]}, \"token.table\": {\"Topic\": [2, 1, 2, 3, 4, 5, 5, 5, 1, 2, 1, 3, 5, 3, 1, 2, 3, 4, 5, 1, 2, 4, 3, 4, 2, 3, 5, 4, 1, 1, 2, 4, 5, 5, 4, 3, 1, 2, 3, 4, 2, 3, 5, 1, 2, 1, 4, 3, 4, 1, 2, 3, 5, 3, 4, 2, 4, 4, 1, 2, 3, 5, 1, 5, 2, 1, 2, 1, 5, 1, 5, 3, 1, 1, 2, 3, 4, 5, 2], \"Freq\": [0.9873604145130433, 0.2817943835141189, 0.12328504278742701, 0.369855128362281, 0.14089719175705945, 0.08806074484816215, 0.9074458274482659, 0.9553591543567632, 0.3913094658144501, 0.604750992622332, 0.9487368411257813, 0.9812703572842992, 0.9570760498336506, 0.8997733254606266, 0.32205217638032396, 0.20340137455599408, 0.16950114546332842, 0.11865080182432988, 0.18645126000966125, 0.8821420039721779, 0.07561217176904382, 0.050408114512695876, 0.7884062478493564, 0.1971015619623391, 0.03879750056312197, 0.8147475118255614, 0.15519000225248789, 0.9626363353754588, 0.9850879387470006, 0.19194538943662598, 0.49357385855132396, 0.10968307967807199, 0.19194538943662598, 0.8811294969057202, 0.9285496488527788, 0.9544735665662906, 0.24854237238773805, 0.4088922900572465, 0.21647238885383635, 0.12026243825213132, 0.9563305363381556, 0.7781283800270912, 0.2122168309164794, 0.11358075001603558, 0.8518556251202668, 0.9519291587819896, 0.9085867343937107, 0.809948798711481, 0.16198975974229618, 0.7195910015763415, 0.15643282642963946, 0.09385969585778368, 0.8893893571323939, 0.9641995083564899, 0.8874075651106887, 0.7443716310521378, 0.24812387701737926, 0.896350875763646, 0.3476557692640378, 0.4671624399485508, 0.18469212742152005, 0.8378933999927082, 0.7932420774093684, 0.18305586401754656, 0.9306061279007951, 0.9748161256426838, 0.9558449877809719, 0.24193663233298465, 0.7258098969989539, 0.805939304450829, 0.1343232174084715, 0.9644205777402206, 0.9699067501844062, 0.18587708262044383, 0.07966160683733307, 0.37175416524088767, 0.34520029629510995, 0.02655386894577769, 0.9600642989358682], \"Term\": [\"also\", \"amoxicillin\", \"amoxicillin\", \"amoxicillin\", \"amoxicillin\", \"amoxicillin\", \"bit\", \"child\", \"day\", \"day\", \"diazepam\", \"dose\", \"far\", \"find\", \"get\", \"get\", \"get\", \"get\", \"get\", \"give\", \"give\", \"give\", \"good\", \"good\", \"help\", \"help\", \"help\", \"hope\", \"hour\", \"just\", \"just\", \"just\", \"just\", \"leave\", \"link\", \"look\", \"lorazepam\", \"lorazepam\", \"lorazepam\", \"lorazepam\", \"much\", \"need\", \"need\", \"pain\", \"pain\", \"patient\", \"people\", \"really\", \"really\", \"say\", \"say\", \"say\", \"shit\", \"short\", \"side_effect\", \"so\", \"so\", \"stop\", \"take\", \"take\", \"take\", \"talk\", \"tell\", \"tell\", \"test\", \"thank\", \"time\", \"today\", \"today\", \"too\", \"too\", \"treatment\", \"try\", \"use\", \"use\", \"use\", \"use\", \"use\", \"very\"]}, \"R\": 5, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 3, 1, 4, 5]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1207222856979607528758286975\", ldavis_el1207222856979607528758286975_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1207222856979607528758286975\", ldavis_el1207222856979607528758286975_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1207222856979607528758286975\", ldavis_el1207222856979607528758286975_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "1      0.201620 -0.013788       1        1  25.351666\n",
       "2      0.073439  0.173693       2        1  24.047779\n",
       "0     -0.173010 -0.106800       3        1  22.690616\n",
       "3     -0.139383  0.120662       4        1  15.389592\n",
       "4      0.037335 -0.173767       5        1  12.520346, topic_info=             Term       Freq       Total Category  logprob  loglift\n",
       "30             so  40.000000   40.000000  Default   5.0000   5.0000\n",
       "42           give  39.000000   39.000000  Default   4.0000   4.0000\n",
       "55           take  92.000000   92.000000  Default   3.0000   3.0000\n",
       "169          need  28.000000   28.000000  Default   2.0000   2.0000\n",
       "224       patient  21.000000   21.000000  Default   1.0000   1.0000\n",
       "224       patient  20.381401   21.009967   Topic1  -4.1695   1.3420\n",
       "372           try   9.711146   10.310270   Topic1  -4.9108   1.3125\n",
       "190         thank   9.659936   10.258345   Topic1  -4.9161   1.3122\n",
       "353          hour   9.548544   10.151378   Topic1  -4.9277   1.3111\n",
       "1220     diazepam   8.893043    9.486298   Topic1  -4.9988   1.3077\n",
       "42           give  34.988469   39.676152   Topic1  -3.6291   1.2466\n",
       "102           too  18.476169   22.334188   Topic1  -4.2676   1.1827\n",
       "53            say  23.357111   31.962601   Topic1  -4.0332   1.0587\n",
       "786          tell  13.348048   16.388440   Topic1  -4.5927   1.1671\n",
       "55           take  31.756201   92.045071   Topic1  -3.7260   0.3081\n",
       "1203    lorazepam  31.495613  124.727223   Topic1  -3.7342  -0.0040\n",
       "16           time  16.135500   16.739116   Topic2  -4.3503   1.3884\n",
       "202          very  11.884730   12.499163   Topic2  -4.6560   1.3747\n",
       "524          also  10.527087   11.140815   Topic2  -4.7773   1.3685\n",
       "365          much   8.784882    9.410972   Topic2  -4.9583   1.3563\n",
       "346          test   7.979664    8.596548   Topic2  -5.0544   1.3507\n",
       "30             so  30.266248   40.302449   Topic2  -3.7213   1.1387\n",
       "547          pain  14.684031   17.608618   Topic2  -4.4445   1.2435\n",
       "55           take  42.776369   92.045071   Topic2  -3.3753   0.6588\n",
       "1203    lorazepam  51.060149  124.727223   Topic2  -3.1983   0.5320\n",
       "105           day  16.874607   28.110743   Topic2  -4.3055   0.9148\n",
       "29           just  18.066901   36.468706   Topic2  -4.2372   0.7228\n",
       "76           dose  12.648057   13.248133   Topic3  -4.5357   1.4369\n",
       "106     treatment  11.823656   12.442704   Topic3  -4.6031   1.4322\n",
       "168          look  10.928130   11.524677   Topic3  -4.6819   1.4301\n",
       "65           find   9.401387   10.002519   Topic3  -4.8323   1.4212\n",
       "479         short   8.699698    9.334168   Topic3  -4.9099   1.4128\n",
       "500          help  20.660694   25.774856   Topic3  -4.0450   1.2621\n",
       "169          need  21.590029   28.272970   Topic3  -4.0010   1.2135\n",
       "35           good  16.226340   20.294106   Topic3  -4.2866   1.2595\n",
       "368        really  15.170000   18.519689   Topic3  -4.3539   1.2837\n",
       "18    amoxicillin  21.409606   56.778988   Topic3  -4.0094   0.5079\n",
       "1203    lorazepam  27.270985  124.727223   Topic3  -3.7674  -0.0371\n",
       "55           take  17.226648   92.045071   Topic3  -4.2267  -0.1926\n",
       "903        people   9.244388    9.905494   Topic4  -4.4609   1.8024\n",
       "401          stop   8.308289    8.925076   Topic4  -4.5677   1.7999\n",
       "618   side_effect   7.277158    7.888146   Topic4  -4.7002   1.7909\n",
       "1024         link   6.936210    7.538638   Topic4  -4.7482   1.7882\n",
       "340          hope   6.621411    7.271697   Topic4  -4.7946   1.7778\n",
       "115           use  12.819500   37.659296   Topic4  -4.1340   0.7939\n",
       "1203    lorazepam  14.668447  124.727223   Topic4  -3.9992  -0.2690\n",
       "30             so   9.568443   40.302449   Topic4  -4.4265   0.4335\n",
       "878         leave   6.198977    6.809442   Topic5  -4.6542   1.9839\n",
       "696         child   5.657829    6.280361   Topic5  -4.7456   1.9734\n",
       "334          shit   5.010341    5.621835   Topic5  -4.8671   1.9627\n",
       "804          talk   5.313106    5.967346   Topic5  -4.8084   1.9617\n",
       "1091          far   4.568786    5.224245   Topic5  -4.9594   1.9438\n",
       "495           bit   4.783730    5.509971   Topic5  -4.9134   1.9365\n",
       "28            get  11.475287   58.996651   Topic5  -4.0384   0.4405\n",
       "32          today   5.515734    8.266627   Topic5  -4.7710   1.6732\n",
       "29           just   6.965925   36.468706   Topic5  -4.5376   0.4224\n",
       "169          need   6.211555   28.272970   Topic5  -4.6522   0.5623, token_table=      Topic      Freq         Term\n",
       "term                              \n",
       "524       2  0.987360         also\n",
       "18        1  0.281794  amoxicillin\n",
       "18        2  0.123285  amoxicillin\n",
       "18        3  0.369855  amoxicillin\n",
       "18        4  0.140897  amoxicillin\n",
       "...     ...       ...          ...\n",
       "115       2  0.079662          use\n",
       "115       3  0.371754          use\n",
       "115       4  0.345200          use\n",
       "115       5  0.026554          use\n",
       "202       2  0.960064         very\n",
       "\n",
       "[79 rows x 3 columns], R=5, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 3, 1, 4, 5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizing the model\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, mds='mmds', R=5)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dc49ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amoxicillin LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c82f68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lauren boebert calling abolishment dept education like chlamydia speaking out against dang\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "am_data = pd.read_excel('Amoxicillin.xlsx')['New Tweet']\n",
    "# get first 90 characters\n",
    "print(am_data[0][0:90])\n",
    "print(type(am_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0febfd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call dept education chlamydia speak danger\n"
     ]
    }
   ],
   "source": [
    "# import en_core_web_sm\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "#     nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    texts_out = []\n",
    "    for text in texts:\n",
    " # Handle NaN values\n",
    "        if pd.isna(text):\n",
    "            text = ''\n",
    "        doc = nlp(text)\n",
    "        new_text = []\n",
    "        for token in doc:\n",
    "            if token.pos_ in allowed_postags:\n",
    "                new_text.append(token.lemma_)\n",
    "        final = ' '.join(new_text)\n",
    "        texts_out.append(final)\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "lemmatized_texts = lemmatization(am_data)\n",
    "print(lemmatized_texts[0][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36ae26a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['call', 'dept', 'education', 'chlamydia', 'speak', 'danger']\n"
     ]
    }
   ],
   "source": [
    "def gen_words(texts):\n",
    "    final = []\n",
    "    for text in texts:\n",
    "        new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "        final.append(new)\n",
    "    return (final)\n",
    "\n",
    "am_data_words = gen_words(lemmatized_texts)\n",
    "print(am_data_words[0][0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41e2bc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['call', 'dept', 'education', 'chlamydia', 'speak', 'danger']\n"
     ]
    }
   ],
   "source": [
    "#Bigrams and Trigrams\n",
    "bigram_phrases = gensim.models.Phrases(am_data_words, min_count=7, threshold=50)\n",
    "trigram_phrases = gensim.models.Phrases(bigram_phrases[am_data_words], threshold=50)\n",
    "\n",
    "bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "# Function to make bigrams\n",
    "def make_bigrams(texts):\n",
    "    return [bigram[doc] for doc in texts]\n",
    "\n",
    "# Function to make trigrams\n",
    "def make_trigram(texts):\n",
    "    return [trigram[bigram[doc]] for doc in texts]\n",
    "\n",
    "data_bigrams = make_bigrams(am_data_words)\n",
    "data_bigrams_trigrams = make_trigram(data_bigrams)\n",
    "\n",
    "# Convert generators to lists\n",
    "data_bigrams_list = list(data_bigrams)\n",
    "data_bigrams_trigrams_list = list(data_bigrams_trigrams)\n",
    "\n",
    "# Check if there are non-empty lists before printing\n",
    "if data_bigrams_trigrams_list:\n",
    "    print(data_bigrams_trigrams_list[0])\n",
    "else:\n",
    "    print(\"No non-empty bigrams and trigrams.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bebe92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF REMOVAL\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "id2word = corpora.Dictionary(data_bigrams_trigrams)\n",
    "\n",
    "texts = data_bigrams_trigrams\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "print(corpus[0][0:10])\n",
    "\n",
    "tfidf = TfidfModel(corpus, id2word=id2word)\n",
    "\n",
    "low_value = 0.03\n",
    "words =[]\n",
    "words_missing_in_tfidf = []\n",
    "for i in range(0, len(corpus)):\n",
    "    bow = corpus[i]\n",
    "    low_value_words = []\n",
    "    tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "    bow_ids = [id for id, value in bow]\n",
    "    low_value_words = [id for pid, value in tfidf[bow] if value < low_value]\n",
    "    drops = low_value_words + words_missing_in_tfidf\n",
    "    for item in drops:\n",
    "        words.append(id2word[item])\n",
    "    words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids] #the words with tfidf score 0 will be missing\n",
    "    \n",
    "    new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "    corpus[i] = new_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3381b9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=3,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd936c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1207222856787795205387220795\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1207222856787795205387220795_data = {\"mdsDat\": {\"x\": [0.007969873125091412, 0.10113565517640079, -0.10910552830149219], \"y\": [0.13898459729959578, -0.06954688571177772, -0.0694377115878181], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [47.99360182158772, 26.408800655786642, 25.59759752262564]}, \"tinfo\": {\"Term\": [\"take\", \"give\", \"use\", \"take\", \"prescribe\", \"so\", \"give\", \"week\", \"amoxicillin\", \"get\", \"prophylaxis\", \"decade\", \"various\", \"use\", \"antibiotic\", \"amoxicillin\", \"cancer\", \"thing\", \"need\", \"drug\", \"get\", \"use\", \"amoxicillin\"], \"Freq\": [27.0, 27.0, 20.0, 27.134200244271927, 10.254748194347794, 8.88130318876355, 25.95050500078271, 16.276115226241554, 62.30866310897834, 26.02461592116388, 6.9436832731349165, 4.250660085747223, 4.249593762691557, 11.9998308740416, 10.494339237814659, 14.958702186144762, 4.81189440237441, 5.29311970440975, 3.9918677504816684, 7.05321642250345, 8.187261371876561, 6.483159736293487, 7.268762290087803], \"Total\": [27.0, 27.0, 20.0, 27.580548115869707, 10.66949562361171, 9.294192794032503, 27.536128124864963, 17.350337576226767, 84.5361275852109, 39.836010761123894, 7.405647874250617, 4.712627956226392, 4.712902202121518, 20.903868767071053, 19.518408789858995, 84.5361275852109, 5.345069220244739, 5.8867073618448345, 4.490661024338164, 11.254590692226383, 39.836010761123894, 20.903868767071053, 84.5361275852109], \"Category\": [\"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"logprob\": [3.0, 2.0, 1.0, -3.8691, -4.8421, -4.9859, -3.9137, -4.3802, -3.0378, -3.9108, -4.6347, -5.1254, -5.1257, -4.0876, -4.2217, -3.8672, -4.9702, -4.8749, -5.157, -4.5878, -4.4387, -4.6721, -4.5577], \"loglift\": [3.0, 2.0, 1.0, 0.7178, 0.6945, 0.6887, 0.6748, 0.6702, 0.429, 0.3084, 1.2671, 1.2283, 1.228, 0.7764, 0.711, -0.4004, 1.2576, 1.2564, 1.2449, 0.8954, -0.2195, 0.1919, -1.0909]}, \"token.table\": {\"Topic\": [1, 2, 3, 1, 2, 3, 3, 2, 1, 2, 3, 1, 2, 3, 1, 3, 3, 1, 2, 1, 1, 3, 1, 2, 3, 2, 1, 2], \"Freq\": [0.7334142427745476, 0.17743892970351957, 0.08280483386164247, 0.20493473843412094, 0.5123368460853023, 0.25616842304265114, 0.9354415806370157, 0.8487833194460307, 0.08885263154800524, 0.2665578946440157, 0.6219684208360367, 0.6526757951720782, 0.1506174911935565, 0.20082332159140867, 0.9442140842060562, 0.03631592631561755, 0.8907374612158624, 0.9372514271311844, 0.9452245257756514, 0.9683466008773356, 0.9789508129631528, 0.8493712516453427, 0.09567606945325413, 0.5740564167195248, 0.2870282083597624, 0.8487339283635879, 0.9221722591682029, 0.05763576619801268], \"Term\": [\"amoxicillin\", \"amoxicillin\", \"amoxicillin\", \"antibiotic\", \"antibiotic\", \"antibiotic\", \"cancer\", \"decade\", \"drug\", \"drug\", \"drug\", \"get\", \"get\", \"get\", \"give\", \"give\", \"need\", \"prescribe\", \"prophylaxis\", \"so\", \"take\", \"thing\", \"use\", \"use\", \"use\", \"various\", \"week\", \"week\"]}, \"R\": 3, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 3, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1207222856787795205387220795\", ldavis_el1207222856787795205387220795_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1207222856787795205387220795\", ldavis_el1207222856787795205387220795_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1207222856787795205387220795\", ldavis_el1207222856787795205387220795_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "1      0.007970  0.138985       1        1  47.993602\n",
       "2      0.101136 -0.069547       2        1  26.408801\n",
       "0     -0.109106 -0.069438       3        1  25.597598, topic_info=             Term       Freq      Total Category  logprob  loglift\n",
       "55           take  27.000000  27.000000  Default   3.0000   3.0000\n",
       "42           give  27.000000  27.000000  Default   2.0000   2.0000\n",
       "115           use  20.000000  20.000000  Default   1.0000   1.0000\n",
       "55           take  27.134200  27.580548   Topic1  -3.8691   0.7178\n",
       "21      prescribe  10.254748  10.669496   Topic1  -4.8421   0.6945\n",
       "30             so   8.881303   9.294193   Topic1  -4.9859   0.6887\n",
       "42           give  25.950505  27.536128   Topic1  -3.9137   0.6748\n",
       "17           week  16.276115  17.350338   Topic1  -4.3802   0.6702\n",
       "18    amoxicillin  62.308663  84.536128   Topic1  -3.0378   0.4290\n",
       "28            get  26.024616  39.836011   Topic1  -3.9108   0.3084\n",
       "1026  prophylaxis   6.943683   7.405648   Topic2  -4.6347   1.2671\n",
       "1022       decade   4.250660   4.712628   Topic2  -5.1254   1.2283\n",
       "1011      various   4.249594   4.712902   Topic2  -5.1257   1.2280\n",
       "115           use  11.999831  20.903869   Topic2  -4.0876   0.7764\n",
       "34     antibiotic  10.494339  19.518409   Topic2  -4.2217   0.7110\n",
       "18    amoxicillin  14.958702  84.536128   Topic2  -3.8672  -0.4004\n",
       "679        cancer   4.811894   5.345069   Topic3  -4.9702   1.2576\n",
       "114         thing   5.293120   5.886707   Topic3  -4.8749   1.2564\n",
       "169          need   3.991868   4.490661   Topic3  -5.1570   1.2449\n",
       "264          drug   7.053216  11.254591   Topic3  -4.5878   0.8954\n",
       "28            get   8.187261  39.836011   Topic3  -4.4387  -0.2195\n",
       "115           use   6.483160  20.903869   Topic3  -4.6721   0.1919\n",
       "18    amoxicillin   7.268762  84.536128   Topic3  -4.5577  -1.0909, token_table=      Topic      Freq         Term\n",
       "term                              \n",
       "18        1  0.733414  amoxicillin\n",
       "18        2  0.177439  amoxicillin\n",
       "18        3  0.082805  amoxicillin\n",
       "34        1  0.204935   antibiotic\n",
       "34        2  0.512337   antibiotic\n",
       "34        3  0.256168   antibiotic\n",
       "679       3  0.935442       cancer\n",
       "1022      2  0.848783       decade\n",
       "264       1  0.088853         drug\n",
       "264       2  0.266558         drug\n",
       "264       3  0.621968         drug\n",
       "28        1  0.652676          get\n",
       "28        2  0.150617          get\n",
       "28        3  0.200823          get\n",
       "42        1  0.944214         give\n",
       "42        3  0.036316         give\n",
       "169       3  0.890737         need\n",
       "21        1  0.937251    prescribe\n",
       "1026      2  0.945225  prophylaxis\n",
       "30        1  0.968347           so\n",
       "55        1  0.978951         take\n",
       "114       3  0.849371        thing\n",
       "115       1  0.095676          use\n",
       "115       2  0.574056          use\n",
       "115       3  0.287028          use\n",
       "1011      2  0.848734      various\n",
       "17        1  0.922172         week\n",
       "17        2  0.057636         week, R=3, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 3, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizing the model\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, mds='mmds', R=3)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d170f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nurofen LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f205f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking box nurofen like what if took too many\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "nu_data = pd.read_excel('Nurofen.xlsx')['New Tweet']\n",
    "# get first 90 characters\n",
    "print(nu_data[0][0:90])\n",
    "print(type(nu_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26502be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "look box nurofen take too many\n"
     ]
    }
   ],
   "source": [
    "# import en_core_web_sm\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "#     nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    texts_out = []\n",
    "    for text in texts:\n",
    " # Handle NaN values\n",
    "        if pd.isna(text):\n",
    "            text = ''\n",
    "        doc = nlp(text)\n",
    "        new_text = []\n",
    "        for token in doc:\n",
    "            if token.pos_ in allowed_postags:\n",
    "                new_text.append(token.lemma_)\n",
    "        final = ' '.join(new_text)\n",
    "        texts_out.append(final)\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "lemmatized_texts = lemmatization(nu_data)\n",
    "print(lemmatized_texts[0][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff2a4214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['look', 'box', 'nurofen', 'take', 'too', 'many']\n"
     ]
    }
   ],
   "source": [
    "def gen_words(texts):\n",
    "    final = []\n",
    "    for text in texts:\n",
    "        new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "        final.append(new)\n",
    "    return (final)\n",
    "\n",
    "nu_data_words = gen_words(lemmatized_texts)\n",
    "print(nu_data_words[0][0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f001ebcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['look', 'box', 'nurofen', 'take', 'too', 'many']\n"
     ]
    }
   ],
   "source": [
    "# Bigrams and Trigrams\n",
    "bigram_phrases = gensim.models.Phrases(nu_data_words, min_count=7, threshold=50)\n",
    "trigram_phrases = gensim.models.Phrases(bigram_phrases[nu_data_words], threshold=50)\n",
    "\n",
    "bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "# Function to make bigrams\n",
    "def make_bigrams(texts):\n",
    "    return [bigram[doc] for doc in texts]\n",
    "\n",
    "# Function to make trigrams\n",
    "def make_trigram(texts):\n",
    "    return [trigram[bigram[doc]] for doc in texts]\n",
    "\n",
    "data_bigrams = make_bigrams(nu_data_words)\n",
    "data_bigrams_trigrams = make_trigram(data_bigrams)\n",
    "\n",
    "# Convert generators to lists\n",
    "data_bigrams_list = list(data_bigrams)\n",
    "data_bigrams_trigrams_list = list(data_bigrams_trigrams)\n",
    "\n",
    "# Check if there are non-empty lists before printing\n",
    "if data_bigrams_trigrams_list:\n",
    "    print(data_bigrams_trigrams_list[0])\n",
    "else:\n",
    "    print(\"No non-empty bigrams and trigrams.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1fb6a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF REMOVAL\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "id2word = corpora.Dictionary(data_bigrams_trigrams)\n",
    "\n",
    "texts = data_bigrams_trigrams\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "print(corpus[0][0:10])\n",
    "\n",
    "tfidf = TfidfModel(corpus, id2word=id2word)\n",
    "\n",
    "low_value = 0.03\n",
    "words =[]\n",
    "words_missing_in_tfidf = []\n",
    "for i in range(0, len(corpus)):\n",
    "    bow = corpus[i]\n",
    "    low_value_words = []\n",
    "    tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "    bow_ids = [id for id, value in bow]\n",
    "    low_value_words = [id for pid, value in tfidf[bow] if value < low_value]\n",
    "    drops = low_value_words + words_missing_in_tfidf\n",
    "    for item in drops:\n",
    "        words.append(id2word[item])\n",
    "    words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids] #the words with tfidf score 0 will be missing\n",
    "    \n",
    "    new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "    corpus[i] = new_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8124f940",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=3,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8bc8b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1207222857337482884192579156\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1207222857337482884192579156_data = {\"mdsDat\": {\"x\": [0.07953904941178572, -0.007467462909008221, -0.07207158650277748], \"y\": [0.03506308867679044, -0.09398591146663855, 0.05892282278984812], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [37.15057897174702, 32.542459800390965, 30.306961227862022]}, \"tinfo\": {\"Term\": [\"say\", \"nurofen\", \"so\", \"time\", \"even\", \"half\", \"help\", \"nurofen\", \"pain\", \"take\", \"clove\", \"same\", \"make\", \"give\", \"nurofen\", \"just\", \"take\", \"need\", \"right\", \"budge\", \"say\", \"day\", \"so\", \"bad\"], \"Freq\": [6.0, 17.0, 5.0, 2.116868475165062, 1.4935794368016668, 1.4915491019954599, 2.7726321429543055, 7.252475488671653, 3.4152743686320846, 4.0489079004853945, 1.4167969489111627, 1.416677922159545, 1.4165144009272281, 2.6297593809528284, 8.753768957968068, 3.852681669585661, 4.459952622481431, 1.9768742755017525, 1.97120026961082, 1.3857183774506543, 4.348215301873311, 2.5717681151067207, 3.1212268240941135, 2.5729322669125483], \"Total\": [6.0, 17.0, 5.0, 2.5335687916098713, 1.8942359428783495, 1.8941105287202018, 3.768612848377286, 17.940394709896683, 6.8378929819552825, 9.893511088082604, 1.8282698544324743, 1.8282708070123315, 1.8282750694954828, 3.682790192814329, 17.940394709896683, 7.319618645654357, 9.893511088082604, 2.3960396059031974, 2.3965061601900044, 1.801872860529907, 6.019915217672011, 3.6301727462740017, 5.5066486323920785, 4.2372538607464705], \"Category\": [\"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"logprob\": [3.0, 2.0, 1.0, -4.5327, -4.8815, -4.8829, -4.2629, -3.3013, -4.0544, -3.8842, -4.8019, -4.8019, -4.8021, -4.1834, -2.9808, -3.8015, -3.6551, -4.3976, -4.4004, -4.7529, -3.6093, -4.1345, -3.9409, -4.134], \"loglift\": [3.0, 2.0, 1.0, 0.8105, 0.7526, 0.7513, 0.6833, 0.0845, 0.296, 0.0968, 0.8677, 0.8676, 0.8675, 0.7858, 0.4051, 0.4808, 0.3259, 1.0015, 0.9984, 0.9312, 0.8685, 0.8491, 0.6261, 0.6949]}, \"token.table\": {\"Topic\": [1, 2, 3, 3, 2, 1, 3, 1, 1, 2, 1, 1, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 3, 2, 1, 2, 3, 1, 3, 1, 2, 3, 1], \"Freq\": [0.2360019089873061, 0.2360019089873061, 0.7080057269619183, 0.554978112998446, 0.546965207338288, 0.27546898450670065, 0.8264069535201021, 0.527917339843351, 0.27153325268193357, 0.8145997580458006, 0.5279522946718809, 0.7960488701543752, 0.26534962338479173, 0.1366191393855878, 0.5464765575423512, 0.2732382787711756, 0.5469636471474463, 0.8347107431248372, 0.39018093599348197, 0.501661203420191, 0.11148026742670913, 0.4387316396902947, 0.4387316396902947, 0.1462438798967649, 0.8345482407779132, 0.5469649223542271, 0.16611529628596905, 0.16611529628596905, 0.6644611851438762, 0.36319731537532357, 0.5447959730629853, 0.4043054042581776, 0.4043054042581776, 0.1010763510645444, 0.7894003141431053], \"Term\": [\"bad\", \"bad\", \"bad\", \"budge\", \"clove\", \"day\", \"day\", \"even\", \"give\", \"give\", \"half\", \"help\", \"help\", \"just\", \"just\", \"just\", \"make\", \"need\", \"nurofen\", \"nurofen\", \"nurofen\", \"pain\", \"pain\", \"pain\", \"right\", \"same\", \"say\", \"say\", \"say\", \"so\", \"so\", \"take\", \"take\", \"take\", \"time\"]}, \"R\": 3, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 1, 3]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1207222857337482884192579156\", ldavis_el1207222857337482884192579156_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1207222857337482884192579156\", ldavis_el1207222857337482884192579156_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1207222857337482884192579156\", ldavis_el1207222857337482884192579156_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "1      0.079539  0.035063       1        1  37.150579\n",
       "0     -0.007467 -0.093986       2        1  32.542460\n",
       "2     -0.072072  0.058923       3        1  30.306961, topic_info=        Term       Freq      Total Category  logprob  loglift\n",
       "60       say   6.000000   6.000000  Default   3.0000   3.0000\n",
       "3    nurofen  17.000000  17.000000  Default   2.0000   2.0000\n",
       "61        so   5.000000   5.000000  Default   1.0000   1.0000\n",
       "107     time   2.116868   2.533569   Topic1  -4.5327   0.8105\n",
       "193     even   1.493579   1.894236   Topic1  -4.8815   0.7526\n",
       "134     half   1.491549   1.894111   Topic1  -4.8829   0.7513\n",
       "26      help   2.772632   3.768613   Topic1  -4.2629   0.6833\n",
       "3    nurofen   7.252475  17.940395   Topic1  -3.3013   0.0845\n",
       "118     pain   3.415274   6.837893   Topic1  -4.0544   0.2960\n",
       "4       take   4.048908   9.893511   Topic1  -3.8842   0.0968\n",
       "275    clove   1.416797   1.828270   Topic2  -4.8019   0.8677\n",
       "190     same   1.416678   1.828271   Topic2  -4.8019   0.8676\n",
       "99      make   1.416514   1.828275   Topic2  -4.8021   0.8675\n",
       "9       give   2.629759   3.682790   Topic2  -4.1834   0.7858\n",
       "3    nurofen   8.753769  17.940395   Topic2  -2.9808   0.4051\n",
       "16      just   3.852682   7.319619   Topic2  -3.8015   0.4808\n",
       "4       take   4.459953   9.893511   Topic2  -3.6551   0.3259\n",
       "58      need   1.976874   2.396040   Topic3  -4.3976   1.0015\n",
       "94     right   1.971200   2.396506   Topic3  -4.4004   0.9984\n",
       "199    budge   1.385718   1.801873   Topic3  -4.7529   0.9312\n",
       "60       say   4.348215   6.019915   Topic3  -3.6093   0.8685\n",
       "73       day   2.571768   3.630173   Topic3  -4.1345   0.8491\n",
       "61        so   3.121227   5.506649   Topic3  -3.9409   0.6261\n",
       "103      bad   2.572932   4.237254   Topic3  -4.1340   0.6949, token_table=      Topic      Freq     Term\n",
       "term                          \n",
       "103       1  0.236002      bad\n",
       "103       2  0.236002      bad\n",
       "103       3  0.708006      bad\n",
       "199       3  0.554978    budge\n",
       "275       2  0.546965    clove\n",
       "73        1  0.275469      day\n",
       "73        3  0.826407      day\n",
       "193       1  0.527917     even\n",
       "9         1  0.271533     give\n",
       "9         2  0.814600     give\n",
       "134       1  0.527952     half\n",
       "26        1  0.796049     help\n",
       "26        3  0.265350     help\n",
       "16        1  0.136619     just\n",
       "16        2  0.546477     just\n",
       "16        3  0.273238     just\n",
       "99        2  0.546964     make\n",
       "58        3  0.834711     need\n",
       "3         1  0.390181  nurofen\n",
       "3         2  0.501661  nurofen\n",
       "3         3  0.111480  nurofen\n",
       "118       1  0.438732     pain\n",
       "118       2  0.438732     pain\n",
       "118       3  0.146244     pain\n",
       "94        3  0.834548    right\n",
       "190       2  0.546965     same\n",
       "60        1  0.166115      say\n",
       "60        2  0.166115      say\n",
       "60        3  0.664461      say\n",
       "61        1  0.363197       so\n",
       "61        3  0.544796       so\n",
       "4         1  0.404305     take\n",
       "4         2  0.404305     take\n",
       "4         3  0.101076     take\n",
       "107       1  0.789400     time, R=3, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 1, 3])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizing the model\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, mds='mmds', R=3)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d7c09df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lorazepam LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7b9f2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lorazepam stimulates il6 production  associated  poor survival outcomes  pancreatic cancer\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "lo_data = pd.read_excel('Lorazepam.xlsx')['New Tweet']\n",
    "# get first 90 characters\n",
    "print(lo_data[0][0:90])\n",
    "print(type(lo_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b9311b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lorazepam stimulate il6 production associate poor survival outcome pancreatic cancer clinical cancer\n"
     ]
    }
   ],
   "source": [
    "# import en_core_web_sm\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "#     nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    texts_out = []\n",
    "    for text in texts:\n",
    " # Handle NaN values\n",
    "        if pd.isna(text):\n",
    "            text = ''\n",
    "        doc = nlp(text)\n",
    "        new_text = []\n",
    "        for token in doc:\n",
    "            if token.pos_ in allowed_postags:\n",
    "                new_text.append(token.lemma_)\n",
    "        final = ' '.join(new_text)\n",
    "        texts_out.append(final)\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "lemmatized_texts = lemmatization(lo_data)\n",
    "print(lemmatized_texts[0][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21f896d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lorazepam', 'stimulate', 'il', 'production', 'associate', 'poor', 'survival', 'outcome', 'pancreatic', 'cancer', 'clinical', 'cancer', 'research', 'cancer', 'research']\n"
     ]
    }
   ],
   "source": [
    "def gen_words(texts):\n",
    "    final = []\n",
    "    for text in texts:\n",
    "        new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "        final.append(new)\n",
    "    return (final)\n",
    "\n",
    "lo_data_words = gen_words(lemmatized_texts)\n",
    "print(lo_data_words[0][0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "32b664e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lorazepam', 'stimulate', 'il', 'production', 'associate', 'poor', 'survival', 'outcome', 'pancreatic_cancer', 'clinical', 'cancer', 'research', 'cancer', 'research']\n"
     ]
    }
   ],
   "source": [
    "# Bigrams and Trigrams\n",
    "bigram_phrases = gensim.models.Phrases(lo_data_words, min_count=7, threshold=50)\n",
    "trigram_phrases = gensim.models.Phrases(bigram_phrases[lo_data_words], threshold=50)\n",
    "\n",
    "bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "# Function to make bigrams\n",
    "def make_bigrams(texts):\n",
    "    return [bigram[doc] for doc in texts]\n",
    "\n",
    "# Function to make trigrams\n",
    "def make_trigram(texts):\n",
    "    return [trigram[bigram[doc]] for doc in texts]\n",
    "\n",
    "data_bigrams = make_bigrams(lo_data_words)\n",
    "data_bigrams_trigrams = make_trigram(data_bigrams)\n",
    "\n",
    "# Convert generators to lists\n",
    "data_bigrams_list = list(data_bigrams)\n",
    "data_bigrams_trigrams_list = list(data_bigrams_trigrams)\n",
    "\n",
    "# Check if there are non-empty lists before printing\n",
    "if data_bigrams_trigrams_list:\n",
    "    print(data_bigrams_trigrams_list[0])\n",
    "else:\n",
    "    print(\"No non-empty bigrams and trigrams.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ff9a090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 2), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 2)]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF REMOVAL\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "id2word = corpora.Dictionary(data_bigrams_trigrams)\n",
    "\n",
    "texts = data_bigrams_trigrams\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "print(corpus[0][0:10])\n",
    "\n",
    "tfidf = TfidfModel(corpus, id2word=id2word)\n",
    "\n",
    "low_value = 0.03\n",
    "words =[]\n",
    "words_missing_in_tfidf = []\n",
    "for i in range(0, len(corpus)):\n",
    "    bow = corpus[i]\n",
    "    low_value_words = []\n",
    "    tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "    bow_ids = [id for id, value in bow]\n",
    "    low_value_words = [id for pid, value in tfidf[bow] if value < low_value]\n",
    "    drops = low_value_words + words_missing_in_tfidf\n",
    "    for item in drops:\n",
    "        words.append(item)\n",
    "    words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids] #the words with tfidf score 0 will be missing\n",
    "    \n",
    "    new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "    corpus[i] = new_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9b0e535",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=3,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8ea2606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your LDA model is stored in lda_model\n",
    "# and the corpus and dictionary are defined as corpus and id2word\n",
    "\n",
    "# Get the top N terms for each topic\n",
    "N = 3  # Adjust the number of terms as needed\n",
    "top_terms_per_topic = {i: [id2word[word_id] for word_id, prob in lda_model.get_topic_terms(i, topn=N)] for i in range(lda_model.num_topics)}\n",
    "\n",
    "# Visualize the model\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, sort_topics=True, mds='mmds', R=3)\n",
    "\n",
    "# Customize the hover tooltip\n",
    "for i in range(lda_model.num_topics):\n",
    "    terms_str = ', '.join(top_terms_per_topic[i])\n",
    "    vis.topic_info.loc[i, 'Terms'] = f'Top 3 Terms: {terms_str}'\n",
    "\n",
    "# Save the visualization to an HTML file\n",
    "pyLDAvis.save_html(vis, 'lda_visualization.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ef0683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
